#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import cv2
import numpy as np
import threading
import time
import requests
import subprocess
from scripts.camera_manager import camera_manager

class SmartBroadcast:
    """æ™ºèƒ½è¯­éŸ³è§£è¯´ç±»"""
    
    def __init__(self):
        print("åˆå§‹åŒ– SmartBroadcast ç±»...")
        self.is_running = False
        self.current_frame = None
        
        self.camera_available = False
        self.simulation_mode = True
        
        self.api_key = "sk-c8d57c40332a43509e927b3fc22ea04f"
        self.api_url = "https://api.deepseek.com/chat/completions"
        
        self.is_playing_audio = False
        self.broadcast_state = "READY"
        self.captured_image = None
        self.ocr_text = ""
        self.ai_response = ""
        self.last_capture_time = 0
        
        # å¸§ç‡ä¼˜åŒ–
        self.last_frame_time = 0
        self.target_fps = 8
        self.frame_interval = 1.0 / self.target_fps
        
        print("âœ“ SmartBroadcast ç±»åˆå§‹åŒ–å®Œæˆ")

    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        print("è°ƒç”¨ start_monitoring æ–¹æ³•")
        if self.is_running: 
            print("ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        # ä½¿ç”¨å…¨å±€æ‘„åƒå¤´ç®¡ç†å™¨
        self.cap = camera_manager.get_camera()
        if self.cap is not None:
            self.camera_available = True
            self.simulation_mode = False
            print("âœ“ è¯­éŸ³è§£è¯´ç³»ç»Ÿè·å–æ‘„åƒå¤´æˆåŠŸ")
        else:
            self.camera_available = False
            self.simulation_mode = True
            print("âš  è¯­éŸ³è§£è¯´ç³»ç»Ÿä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
        
        self.is_running = True
        self.monitor_thread = threading.Thread(target=self.monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        print("âœ“ å¼€å§‹æ™ºèƒ½è¯­éŸ³è§£è¯´ç›‘æ§...")

    def capture_image(self):
        """æ‹æ‘„å›¾ç‰‡"""
        print("è°ƒç”¨ capture_image æ–¹æ³•")
        if self.broadcast_state != "READY":
            return False, "ç³»ç»Ÿå¿™ï¼Œè¯·ç¨åå†è¯•"
        
        try:
            self.broadcast_state = "CAPTURING"
            
            # è·å–æœ€æ–°å¸§
            frame = self.capture_frame()
            if frame is not None:
                self.captured_image = frame.copy()
            
            self.last_capture_time = time.time()
            self.broadcast_state = "PROCESSING"
            
            # åœ¨æ–°çº¿ç¨‹ä¸­å¤„ç†å›¾ç‰‡
            processing_thread = threading.Thread(target=self.process_image)
            processing_thread.daemon = True
            processing_thread.start()
            
            return True, "å›¾ç‰‡æ‹æ‘„æˆåŠŸï¼Œæ­£åœ¨åˆ†æ..."
            
        except Exception as e:
            self.broadcast_state = "READY"
            return False, f"æ‹æ‘„å¤±è´¥: {str(e)}"

    def get_frame_bytes(self):
        """è·å–å½“å‰å¸§çš„å­—èŠ‚æ•°æ®"""
        print("è°ƒç”¨ get_frame_bytes æ–¹æ³•")
        try:
            # å¦‚æœå½“å‰å¸§ä¸ºç©ºï¼Œå…ˆç”Ÿæˆä¸€å¸§
            if self.current_frame is None:
                print("å½“å‰å¸§ä¸ºç©ºï¼Œç”Ÿæˆæ–°å¸§...")
                frame = self.capture_frame()
                if frame is not None:
                    self.current_frame = self.add_status_overlay(frame)
            
            if self.current_frame is not None:
                # ç¼–ç ä¸ºJPEG
                ret, jpeg = cv2.imencode('.jpg', self.current_frame, [
                    cv2.IMWRITE_JPEG_QUALITY, 70
                ])
                if ret:
                    print(f"âœ“ æˆåŠŸç¼–ç å¸§ï¼Œå¤§å°: {len(jpeg.tobytes())} å­—èŠ‚")
                    return jpeg.tobytes()
                else:
                    print("âœ— JPEGç¼–ç å¤±è´¥")
            else:
                print("âœ— å½“å‰å¸§ä»ç„¶ä¸ºç©º")
                
        except Exception as e:
            print(f"âœ— get_frame_bytesé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        
        # è¿”å›é”™è¯¯å¸§
        try:
            error_frame = np.zeros((480, 640, 3), dtype=np.uint8)
            cv2.putText(error_frame, "è§†é¢‘æµé”™è¯¯", (200, 240), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            ret, jpeg = cv2.imencode('.jpg', error_frame)
            if ret:
                return jpeg.tobytes()
        except:
            pass
        
        return None

    def get_status(self):
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            "broadcast_state": self.broadcast_state,
            "is_playing_audio": self.is_playing_audio,
            "last_capture_time": self.last_capture_time,
            "ocr_text": self.ocr_text,
            "ai_response": self.ai_response,
            "camera_available": self.camera_available,
            "simulation_mode": self.simulation_mode,
            "is_running": self.is_running
        }

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.is_running = False
        camera_manager.release_camera()
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1.0)
        print("æ™ºèƒ½è¯­éŸ³è§£è¯´ç›‘æ§å·²åœæ­¢")

    def monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        frame_count = 0
        last_fps_time = time.time()
        
        while self.is_running:
            current_time = time.time()
            
            if current_time - self.last_frame_time >= self.frame_interval:
                frame = self.capture_frame()
                if frame is not None:
                    self.current_frame = self.add_status_overlay(frame)
                self.last_frame_time = current_time
                frame_count += 1
            
            if current_time - last_fps_time >= 3.0:
                fps = frame_count / (current_time - last_fps_time)
                if frame_count % 15 == 0:
                    print(f"æ’­æŠ¥å¤„ç†FPS: {fps:.1f}")
                frame_count = 0
                last_fps_time = current_time
            
            time.sleep(0.01)

    def capture_frame(self):
        """æ•è·å¸§"""
        if self.simulation_mode or not self.camera_available:
            return self.generate_simulation_frame()
        
        try:
            ret, frame = camera_manager.read_frame()
            if ret and frame is not None:
                return frame
        except Exception as e:
            print(f"æ•è·å¸§é”™è¯¯: {e}")
        
        return self.generate_simulation_frame()

    def generate_simulation_frame(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿå¸§"""
        frame = np.zeros((480, 640, 3), dtype=np.uint8)
        status_color = (0, 255, 0) if self.broadcast_state == "READY" else (255, 255, 0)
        cv2.putText(frame, f"çŠ¶æ€: {self.broadcast_state}", (50, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)
        cv2.putText(frame, "æ¨¡æ‹Ÿæ¨¡å¼ - ç‚¹å‡»æ‹æ‘„æµ‹è¯•", (50, 100), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        return frame

    def add_status_overlay(self, frame):
        """åœ¨å¸§ä¸Šæ·»åŠ çŠ¶æ€ä¿¡æ¯"""
        status_colors = {
            "READY": (0, 255, 0), "CAPTURING": (255, 255, 0), 
            "PROCESSING": (255, 165, 0), "SPEAKING": (0, 255, 255)
        }
        color = status_colors.get(self.broadcast_state, (255, 255, 255))
        
        cv2.putText(frame, f"çŠ¶æ€: {self.broadcast_state}", (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        mode_text = "æ¨¡æ‹Ÿæ¨¡å¼" if self.simulation_mode else "å®æ—¶æ¨¡å¼"
        cv2.putText(frame, f"æ¨¡å¼: {mode_text}", (10, 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        if self.last_capture_time > 0:
            time_str = time.strftime("%H:%M:%S", time.localtime(self.last_capture_time))
            cv2.putText(frame, f"æœ€åæ‹æ‘„: {time_str}", (10, 90), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        if self.is_playing_audio:
            cv2.putText(frame, "ğŸ”Š æ’­æ”¾ä¸­...", (10, 120), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        return frame

    def process_image(self):
        """å¤„ç†å›¾ç‰‡"""
        try:
            print("å¼€å§‹å¤„ç†å›¾ç‰‡...")
            self.ocr_text = "æ™ºèƒ½è¯†åˆ«åˆ°æ™¯åŒºåœºæ™¯"
            self.ai_response = "æ¬¢è¿æ¥åˆ°ç¾ä¸½çš„æ™¯åŒºï¼è¿™é‡Œé£æ™¯ä¼˜ç¾ï¼Œå†å²æ‚ ä¹…..."
            self.speak_text(self.ai_response)
            self.broadcast_state = "READY"
        except Exception as e:
            print(f"å¤„ç†å›¾ç‰‡é”™è¯¯: {e}")
            self.broadcast_state = "READY"

    def speak_text(self, text):
        """æ–‡å­—è½¬è¯­éŸ³"""
        try:
            self.broadcast_state = "SPEAKING"
            self.is_playing_audio = True
            
            clean_text = text[:100]
            cmd = ['espeak', '-v', 'zh', '-s', '150', clean_text]
            subprocess.run(cmd, timeout=30)
        except Exception as e:
            print(f"è¯­éŸ³æ’­æ”¾é”™è¯¯: {e}")
        finally:
            self.is_playing_audio = False

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop_monitoring()